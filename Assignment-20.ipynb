{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19f842-9955-46c5-b41c-20d49da8b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "Underlying Concept of SVM:\n",
    "Support Vector Machines (SVM) are supervised learning models used for classification and regression tasks. \n",
    "The fundamental concept is to find the hyperplane that best separates data points of different classes in the feature space. \n",
    "SVM aims to maximize the margin between the closest data points (support vectors) of different classes and the hyperplane, \n",
    "thus achieving the best generalization for the classifier.\n",
    "\n",
    "# 2. What is the concept of a support vector?\n",
    "\n",
    "Concept of a Support Vector:\n",
    "Support vectors are the data points that are closest to the decision boundary or hyperplane. These points are crucial \n",
    "in defining the position and orientation of the hyperplane because the SVM algorithm optimizes the margin based on \n",
    "these support vectors. Any other points that do not influence the margin are ignored during training.\n",
    "\n",
    "# 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "Necessity of Scaling Inputs in SVM:\n",
    "Scaling the inputs is necessary when using SVMs because the algorithm is sensitive to the scale of the input features. \n",
    "If the features are on different scales, SVM may give more importance to features with larger values, leading to \n",
    "suboptimal hyperplanes. Scaling ensures that all features contribute equally to the model, improving convergence and performance.\n",
    "\n",
    "# 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "Confidence Score and Percentage Chance in SVM:\n",
    "SVMs do not directly provide a probability or percentage chance of a case belonging to a particular class. However, \n",
    "the distance of a data point from the decision boundary can be interpreted as a confidence score. \n",
    "To get a probability or percentage chance, you can use methods like Platt scaling or the `probability=True` parameter in Scikit-Learn's SVC.\n",
    "\n",
    "# 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "Primal vs Dual Form for Large Datasets:\n",
    "For a large dataset with millions of instances and hundreds of features, it is typically better to use the primal form \n",
    "of the SVM problem, especially if the number of features is greater than the number of instances. The dual form is more \n",
    "suited for cases where the number of features is smaller than the number of instances.\n",
    "\n",
    "# 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower gamma? What about the letter C?\n",
    "\n",
    "Adjusting gamma and C for Underfitting:\n",
    "If your SVM with an RBF kernel is underfitting, you should try increasing the gamma parameter. A higher gamma value \n",
    "makes the decision boundary more complex, which can help in capturing the patterns in the data. Similarly, increasing \n",
    "the C parameter will reduce the regularization strength, allowing the model to fit the training data more closely.\n",
    "\n",
    "# 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "Setting QP Parameters for Soft Margin SVM:\n",
    "For a soft margin linear SVM classifier:\n",
    "- H is set to a positive semi-definite matrix, which corresponds to the identity matrix with the number of features.\n",
    "- f is a vector of zeros.\n",
    "- A represents the constraints matrix with labels (y_i) multiplied by feature vectors (x_i).\n",
    "- b is a vector of ones, representing the margin constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab8f074-557a-4aa3-898b-d2ce3c9214a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9, 0.925, 0.915)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "# Training Different Classifiers on a Linearly Separable Dataset:\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a linearly separable dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a LinearSVC\n",
    "linear_svc = LinearSVC(random_state=42)\n",
    "linear_svc.fit(X_train, y_train)\n",
    "linear_svc_pred = linear_svc.predict(X_test)\n",
    "\n",
    "# Train an SVC with a linear kernel\n",
    "svc = SVC(kernel=\"linear\", random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "# Train an SGDClassifier\n",
    "sgd_clf = SGDClassifier(loss=\"hinge\", random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_pred = sgd_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of each model\n",
    "linear_svc_acc = accuracy_score(y_test, linear_svc_pred)\n",
    "svc_acc = accuracy_score(y_test, svc_pred)\n",
    "sgd_acc = accuracy_score(y_test, sgd_pred)\n",
    "\n",
    "linear_svc_acc, svc_acc, sgd_acc  # All should have similar accuracy on a linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9737f-2116-4130-a7e8-65725abcbf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cla.shehal\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV 1/3] END ..................C=1, gamma=0.001;, score=0.109 total time=  25.1s\n",
      "[CV 2/3] END ..................C=1, gamma=0.001;, score=0.109 total time=  27.3s\n",
      "[CV 3/3] END ..................C=1, gamma=0.001;, score=0.109 total time=  35.0s\n",
      "[CV 1/3] END ...................C=1, gamma=0.01;, score=0.109 total time=  21.7s\n",
      "[CV 2/3] END ...................C=1, gamma=0.01;, score=0.109 total time=  19.2s\n",
      "[CV 3/3] END ...................C=1, gamma=0.01;, score=0.109 total time=  21.0s\n",
      "[CV 1/3] END ....................C=1, gamma=0.1;, score=0.109 total time=  20.5s\n",
      "[CV 2/3] END ....................C=1, gamma=0.1;, score=0.109 total time=  22.0s\n",
      "[CV 3/3] END ....................C=1, gamma=0.1;, score=0.109 total time=  19.6s\n",
      "[CV 1/3] END .................C=10, gamma=0.001;, score=0.109 total time=  18.5s\n",
      "[CV 2/3] END .................C=10, gamma=0.001;, score=0.109 total time=  18.4s\n",
      "[CV 3/3] END .................C=10, gamma=0.001;, score=0.109 total time=  20.6s\n",
      "[CV 1/3] END ..................C=10, gamma=0.01;, score=0.109 total time=  29.5s\n",
      "[CV 2/3] END ..................C=10, gamma=0.01;, score=0.109 total time=  32.6s\n",
      "[CV 3/3] END ..................C=10, gamma=0.01;, score=0.109 total time=  27.3s\n",
      "[CV 1/3] END ...................C=10, gamma=0.1;, score=0.109 total time=  24.6s\n",
      "[CV 2/3] END ...................C=10, gamma=0.1;, score=0.109 total time=  22.6s\n",
      "[CV 3/3] END ...................C=10, gamma=0.1;, score=0.109 total time=  23.7s\n",
      "[CV 1/3] END ................C=100, gamma=0.001;, score=0.109 total time=  28.5s\n",
      "[CV 2/3] END ................C=100, gamma=0.001;, score=0.109 total time=  22.7s\n",
      "[CV 3/3] END ................C=100, gamma=0.001;, score=0.109 total time=  21.9s\n",
      "[CV 1/3] END .................C=100, gamma=0.01;, score=0.109 total time=  23.2s\n",
      "[CV 2/3] END .................C=100, gamma=0.01;, score=0.109 total time=  21.9s\n",
      "[CV 3/3] END .................C=100, gamma=0.01;, score=0.109 total time=  19.3s\n",
      "[CV 1/3] END ..................C=100, gamma=0.1;, score=0.109 total time=  21.4s\n",
      "[CV 2/3] END ..................C=100, gamma=0.1;, score=0.109 total time=  22.6s\n"
     ]
    }
   ],
   "source": [
    "# 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "# Training an SVM on the MNIST Dataset:\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use a small subset for hyperparameter tuning\n",
    "small_train_X, small_train_y = X_train[:5000], y_train[:5000]\n",
    "\n",
    "# Grid search for best hyperparameters\n",
    "param_grid = {'C': [1, 10, 100], 'gamma': [0.001, 0.01, 0.1]}\n",
    "grid_search = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=3, verbose=3)\n",
    "grid_search.fit(small_train_X, small_train_y)\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "best_svm = grid_search.best_estimator_\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mnist_accuracy = best_svm.score(X_test, y_test)\n",
    "mnist_accuracy  # Expected accuracy around 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185a31c4-e35f-42b7-b611-c958e7c01774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7275639524733043"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "# Training an SVM Regressor on the California Housing Dataset:\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline to scale the data and train an SVM regressor\n",
    "svm_reg = make_pipeline(StandardScaler(), SVR(kernel=\"rbf\", C=1, gamma=\"scale\"))\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "housing_score = svm_reg.score(X_test, y_test)\n",
    "housing_score  # Expected R^2 score around 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1a3af-fd7b-4129-b9e6-b3e8dbcaea59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
